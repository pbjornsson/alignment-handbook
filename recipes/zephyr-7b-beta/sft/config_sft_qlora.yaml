# Model arguments
model_name_or_path: mistralai/Mistral-7B-v0.1
torch_dtype: float16

# LoRA arguments
use_peft: true
lora_r: 64
lora_alpha: 16
lora_dropout: 0.1
lora_target_modules:
- q_proj
- v_proj
- k_proj
- o_proj

# Use 4-bit quantization
load_in_4bit: true
bnb_4bit_compute_dtype: float16
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true

# Data arguments
dataset_mixer:
  HuggingFaceH4/ultrachat_200k: 1.0
dataset_splits:
- train_sft
preprocessing_num_workers: 2

# Training arguments
output_dir: ./results/mistral-7b-qlora
num_train_epochs: 1
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2.0e-04
warmup_steps: 100
logging_steps: 10
save_steps: 100
fp16: true
gradient_checkpointing: true


# with open('recipes/qlora_config.yaml', 'w') as f:
#     f.write(qlora_config)